# ç‰¹æƒæ•™å¸ˆï¼š
## è§‚æµ‹ï¼š
ä¸‹é¢åŸºäºä½ å½“å‰çš„é…ç½®ï¼ˆhistory_length=5ï¼Œconcatenate_terms=Trueï¼ŒåŒ…å« height_scannerï¼‰ç»™å‡ºâ€œå®é™…è¿›å…¥ rsl_rl çš„æœ€ç»ˆ obsâ€çš„ç»„æˆã€é¡ºåºã€æ¯é¡¹ shapeï¼Œä»¥åŠæ€»ç»´åº¦å¦‚ä½•è®¡ç®—ï¼Œå¹¶æä¾›ä¸€æ®µå¯ç›´æ¥è¿è¡Œçš„æ£€æŸ¥è„šæœ¬ï¼Œæ‰“å°å„æˆåˆ†ä¸æœ€ç»ˆæ‹¼æ¥åçš„ shapeã€‚

ä¸€ã€æ¯æ­¥ï¼ˆå•å¸§ï¼‰åŸå­è§‚æµ‹é¡¹ä¸ç»´åº¦
è®°ï¼š
- n_j = æœºå™¨äººå…³èŠ‚æ•°ï¼ˆGo2 é€šå¸¸æ˜¯ 12ï¼‰
- n_h = é«˜åº¦é›·å°„æ‰«æé•¿åº¦ï¼ˆç”± RayCaster ç½‘æ ¼å†³å®šï¼‰
- height_scanner çš„ç½‘æ ¼ç”± patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]) ç»™å‡ºï¼Œn_h â‰ˆ (floor(1.6/0.1)+1) * (floor(1.0/0.1)+1) = 17*11 = 187ï¼ˆä»¥å®é™…ä¼ æ„Ÿå™¨å®ç°ä¸ºå‡†ï¼Œä¸‹é¢ç»™å‡ºä»£ç æ£€æŸ¥ï¼‰
- å…¶å®ƒé¡¹éƒ½æ˜¯æ ‡é‡æˆ–å›ºå®šå‘é‡ï¼Œä¸æ”¹å˜ shape

æŒ‰åœ¨ ObservationsCfg.PolicyCfg ä¸­å®šä¹‰çš„é¡ºåºï¼ˆorder preservedï¼‰ï¼Œå•å¸§å„é¡¹ç»´åº¦ä¸ºï¼š
- base_lin_vel: 3
- base_ang_vel: 3
- projected_gravity: 3
- velocity_commands: 3
- joint_pos_rel: n_j
- joint_vel_rel: n_j
- last_action: n_j
- height_scanner: n_h

å› æ­¤ï¼Œå•å¸§â€œpolicyâ€è§‚æµ‹ç»´åº¦
- Ao_per_step = 3+3+3+3 + n_j + n_j + n_j + n_h = 12 + 3*n_j + n_h
- å¯¹ Go2ï¼ˆn_j=12ï¼‰ï¼šAo_per_step = 48 + n_h

â€œcriticâ€ç»„ï¼ˆæ¯” policy å¤šäº† joint_effortï¼‰å•å¸§ç»´åº¦
- Co_per_step = 3+3+3+3 + n_j + n_j + n_j + n_j + n_h = 12 + 4*n_j + n_h
- å¯¹ Go2ï¼šCo_per_step = 60 + n_h

äºŒã€history_length=5 ä¸æ‹¼æ¥åçš„æœ€ç»ˆ obs
- ç”±äº ObservationsCfg.PolicyCfg.history_length = 5 ä¸” concatenate_terms = Trueï¼Œç®¡ç†å™¨ä¼šæŠŠæœ€è¿‘ 5 å¸§æŒ‰æ—¶é—´é¡ºåºä¸²æ¥åœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Šï¼ˆé€šå¸¸ä»æœ€æ—§åˆ°æœ€æ–°ï¼Œæœ€åä¸€æ®µä¸ºå½“å‰å¸§ï¼‰ã€‚
- æœ€ç»ˆ policy/critic çš„è§‚æµ‹ç»´åº¦ï¼š
  - Ao = history_length * Ao_per_step = 5 * (12 + 3*n_j + n_h)
  - Co = history_length * Co_per_step = 5 * (12 + 4*n_j + n_h)

ä¸‰ã€ä¸ TerrainAwareActorCritic çš„å¯¹æ¥ä¸åˆ‡åˆ†
- è¯¥ç½‘ç»œæŠŠâ€œæœ€å height_obs_dim ä¸ªå…ƒç´ â€å½“ä½œåœ°å½¢é«˜åº¦è¾“å…¥ height_scannerï¼Œå¹¶èµ°åœ°å½¢ MLP åˆ†æ”¯ï¼›å…¶ä½™ä½œä¸ºâ€œcoreâ€èµ° RNN å’Œç›´è¿ã€‚
- å› ä¸º history æ‹¼æ¥åœ¨æœ€åç»´åº¦ä¸Šï¼Œä¸” height_scanner åœ¨æ¯å¸§çš„æœ€åä¸€é¡¹ï¼Œæ•…â€œæœ€å n_h ä¸ªå…ƒç´ â€æ°å¥½å¯¹åº”â€œæœ€æ–°ä¸€å¸§çš„ height_scannerâ€ï¼ˆè¿™æ­£æ˜¯æ¨¡å‹æœŸæœ›çš„é«˜åº¦è¾“å…¥ï¼‰ã€‚
- æ³¨æ„ï¼šå‰ 4 å¸§çš„ height_scanner ä¼šè½åœ¨â€œcoreâ€é‚£ä¸€å¤§æ®µé‡Œï¼Œä¸€å¹¶è¾“å…¥ RNNï¼ˆè¿™æ˜¯è¯¥å®ç°çš„è®¾è®¡é€‰æ‹©ï¼šä»…æœ€æ–°å¸§èµ°åœ°å½¢ MLPï¼Œå†å²çš„åœ°å½¢æ•°æ®è¿›å…¥ RNNï¼‰ã€‚

å› æ­¤ï¼Œç»™ TerrainAwareActorCritic çš„æœŸæœ›è¾“å…¥å½¢å¼ä¸ shape ä¸ºï¼š
- act(observations): observations å½¢çŠ¶ (B, Ao) æˆ– (T, B, Ao)
- evaluate(critic_observations): å½¢çŠ¶ (B, Co) æˆ– (T, B, Co)
- å…¶ä¸­ Ao ä¸ Co æŒ‰ä¸Šä¸€èŠ‚å…¬å¼è®¡ç®—ï¼›height_obs_dim åº”è®¾ç½®ä¸º n_hï¼ˆä¸æ˜¯ 5*n_hï¼‰

å››ã€å¦‚ä½•ç›´è§‚æ£€æŸ¥å„æˆåˆ† shape ä¸æœ€ç»ˆ obs shape
ä¸‹é¢çš„è„šæœ¬ä¼šï¼š
- åˆ›å»ºç¯å¢ƒå¹¶ reset
- æ¨æ–­ n_j ä¸ n_h
- è®¡ç®— Ao_per_step/Co_per_step ä¸ Ao/Co
- æŠŠ policy/critic çš„æ‹¼æ¥è§‚æµ‹æŒ‰â€œæ—¶é—´å—â€ï¼ˆhistory_length ä¸ª blockï¼‰ä¸â€œé¡¹å†…é¡ºåºâ€åˆ‡ç‰‡ï¼Œæ‰“å°æ¯é¡¹åœ¨â€œæœ€æ–°ä¸€å¸§â€çš„åˆ‡ç‰‡ shapeï¼Œå¹¶éªŒè¯â€œæœ€å n_h å…ƒç´ â€ä¸â€œæœ€æ–°ä¸€å¸§çš„ height_scanner åˆ‡ç‰‡â€ä¸€è‡´

ä½ å¯ä»¥å°†å…¶ä¿å­˜ä¸ºä¸€ä¸ªä¸´æ—¶è„šæœ¬å¹¶åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œï¼ˆæˆ–ç²˜åˆ°ä¸€ä¸ª Notebook å•å…ƒè¿è¡Œï¼‰ã€‚

````python
# ç”¨äºç›´è§‚æ£€æŸ¥æœ€ç»ˆ obs çš„ç»„æˆä¸ shape

import torch
from isaaclab.envs import ManagerBasedRLEnv
from unitree_rl_lab.tasks.locomotion.robots.go2.velocity_env_cfg import RobotPlayEnvCfg, RobotEnvCfg

def infer_num_joints(env):
    # å¤šç§å…¼å®¹æ–¹å¼å°è¯•è·å–å…³èŠ‚æ•°
    robot = getattr(env.scene, "robot", None)
    if robot is not None and hasattr(robot, "num_dof"):
        return int(robot.num_dof)
    if hasattr(env.scene, "articulations") and "robot" in env.scene.articulations:
        art = env.scene.articulations["robot"]
        if hasattr(art, "num_dof"):
            return int(art.num_dof)
    # å…œåº•ï¼šä» action ç»´åº¦å€’æ¨
    obs = env.reset()
    act_dim = env.action_manager.action_spec.num_actions
    return int(act_dim)

def infer_height_dim(env):
    # ä¼˜å…ˆä»ä¼ æ„Ÿå™¨æ‹¿ï¼›å¦‚ä¸å¯å¾—ï¼ŒæŒ‰ size/resolution ä¼°è®¡
    sensor = env.scene.sensors.get("height_scanner", None)
    if sensor is not None:
        # å¸¸è§å±æ€§å°è¯•
        for attr in ["num_rays", "num_beams", "ray_count"]:
            if hasattr(sensor, attr):
                return int(getattr(sensor, attr))
        # ä¼°ç®—
        pcfg = sensor.cfg.pattern_cfg
        sx, sy = pcfg.size[0], pcfg.size[1]
        res = pcfg.resolution
        nx = int(round(sx / res)) + 1
        ny = int(round(sy / res)) + 1
        return nx * ny
    # å…œåº•ï¼šä»æ‹¼æ¥å‘é‡æœ«å°¾æ¨æ–­ï¼ˆéœ€çŸ¥é“å…¶å®ƒé¡¹ç»´åº¦ï¼‰
    raise RuntimeError("æ— æ³•ä»ä¼ æ„Ÿå™¨ç›´æ¥æ¨æ–­ n_hï¼Œè¯·æŒ‰ä¸‹æ–¹æ‰“å°ç»“æœäººå·¥æ ¡éªŒã€‚")

def split_last_frame_terms(vec_last_frame, n_j, n_h, is_critic=False):
    # æŒ‰ ObservationsCfg ä¸­å®šä¹‰é¡ºåºåˆ‡ç‰‡ï¼ˆå•å¸§ï¼‰
    idx = 0
    out = {}
    def take(k):
        nonlocal idx
        s = vec_last_frame[..., idx:idx+k]
        idx += k
        return s

    out["base_lin_vel"]      = take(3)
    out["base_ang_vel"]      = take(3)
    out["projected_gravity"] = take(3)
    out["velocity_commands"] = take(3)
    out["joint_pos_rel"]     = take(n_j)
    out["joint_vel_rel"]     = take(n_j)
    if is_critic:
        out["joint_effort"]  = take(n_j)
    out["last_action"]       = take(n_j)
    out["height_scanner"]    = take(n_h)
    assert idx == vec_last_frame.shape[-1], f"å¸§åˆ‡ç‰‡æœªå¯¹é½ï¼Œå·²å– {idx}, ä½†æ€»é•¿ {vec_last_frame.shape[-1]}"
    return out

def main():
    cfg = RobotPlayEnvCfg()  # æˆ– RobotEnvCfg() ç”¨äºè®­ç»ƒé…ç½®
    env = ManagerBasedRLEnv(cfg)

    obs = env.reset()  # obs æ˜¯ dict: {"policy": (N, Ao), "critic": (N, Co)}
    policy = obs["policy"]
    critic = obs["critic"]
    N, Ao = policy.shape
    _, Co = critic.shape

    n_j = infer_num_joints(env)
    n_h = infer_height_dim(env)
    H = cfg.observations.policy.history_length

    Ao_per_step = 12 + 3*n_j + n_h
    Co_per_step = 12 + 4*n_j + n_h

    print(f"num_envs={N}, n_j={n_j}, n_h={n_h}, history_length={H}")
    print(f"Ao_per_step={Ao_per_step}, Ao={Ao} (æœŸæœ› {H * Ao_per_step})")
    print(f"Co_per_step={Co_per_step}, Co={Co} (æœŸæœ› {H * Co_per_step})")

    # è¿˜åŸä¸º (N, H, per_step)
    policy_blocks = policy.view(N, H, Ao_per_step)
    critic_blocks = critic.view(N, H, Co_per_step)
    # å–æœ€æ–°ä¸€å¸§ï¼ˆé€šå¸¸æœ€åä¸€ä¸ªå—ï¼‰
    pol_last = policy_blocks[:, -1, :]
    cri_last = critic_blocks[:, -1, :]

    # æŒ‰å•å¸§é¡ºåºåˆ‡å¼€å„é¡¹
    pol_terms = split_last_frame_terms(pol_last, n_j, n_h, is_critic=False)
    cri_terms = split_last_frame_terms(cri_last, n_j, n_h, is_critic=True)

    print("\nPolicy(æœ€åä¸€å¸§)å„é¡¹shapeï¼š")
    for k, v in pol_terms.items():
        print(f"  {k:>18s}: {tuple(v.shape)}")

    print("\nCritic(æœ€åä¸€å¸§)å„é¡¹shapeï¼š")
    for k, v in cri_terms.items():
        print(f"  {k:>18s}: {tuple(v.shape)}")

    # éªŒè¯â€œæœ€å n_h å…ƒç´ â€ç¡®ä¸ºæœ€æ–°ä¸€å¸§ height_scanner
    tail_height = policy[:, -n_h:]
    assert torch.allclose(tail_height, pol_terms["height_scanner"], atol=0, rtol=0), \
        "æœ€å n_h å…ƒç´ ä¸ç­‰äºæœ€æ–°ä¸€å¸§çš„ height_scannerï¼ˆåˆ‡åˆ†æˆ–é¡ºåºä¸å‡è®¾ä¸ç¬¦ï¼‰"
    print("\næ ¡éªŒé€šè¿‡ï¼špolicy å‘é‡æœ«å°¾çš„ n_h ç¡®ä¸ºæœ€æ–°ä¸€å¸§ height_scannerã€‚")

    # å¦‚éœ€æŸ¥çœ‹æ¯ä¸ªæ—¶é—´å—ï¼ˆå†å²æ¯ä¸€å¸§ï¼‰ä¸­ height_scanner çš„ shapeï¼š
    heights_over_time = policy_blocks[..., -n_h:]  # (N, H, n_h)
    print(f"\nheight_scanner over time shape: {tuple(heights_over_time.shape)}  (åº”ä¸º (N, {H}, n_h))")

if __name__ == "__main__":
    main()
````

äº”ã€è¦ç‚¹å°ç»“
- æœ€ç»ˆ obs é¡ºåºæ˜¯â€œæŒ‰å¸§æ‹¼æ¥ã€å¸§å†…æŒ‰ term é¡ºåºæ‹¼æ¥â€ï¼Œheight_scanner åœ¨æ¯å¸§æœ«å°¾ï¼›å› æ­¤ obs å‘é‡çš„â€œæœ€å n_h ä¸ªå…ƒç´ â€å¯¹åº”â€œæœ€æ–°ä¸€å¸§çš„ height_scannerâ€ï¼Œå¯ç›´æ¥ä½œä¸º height_obs_dim è¾“å…¥ç»™ TerrainAwareActorCriticã€‚
- å‰ H-1 å¸§çš„ height_scanner ä¼šè¢«åŒ…å«åœ¨â€œcoreâ€éƒ¨åˆ†ï¼Œè¿›å…¥ RNNï¼ˆè®¾è®¡ä¸Šç”¨ RNNå»ºæ¨¡å†å²åœ°å½¢+æœ¬ä½“ä¿¡æ¯ï¼›ä»…æœ€æ–°åœ°å½¢ç»è¿‡ä¸“ç”¨ MLPï¼‰ã€‚
- Go2 å¸¸è§è®¾ç½®ä¸‹ï¼šAo_per_step = 48 + n_hï¼ŒCo_per_step = 60 + n_hï¼›æœ€ç»ˆ Ao=5*(48+n_h)ã€Co=5*(60+n_h)ã€‚å®é™… n_h è¯·ä»¥è„šæœ¬æ‰“å°ä¸ºå‡†ã€‚


# padä¸RNN
å¥½ï¼Œæˆ‘ç”¨ **æœ€å¥½ç†è§£çš„ä¾‹å­ï¼ˆå«å›¾è§£ï¼‰** å¸®ä½ ç†è§£ **pad çš„ä½œç”¨**ï¼Œç‰¹åˆ«æ˜¯åœ¨ IsaacLab / RSL-RL / RNN Memory ä¸­å¤„ç† **å†å²åºåˆ—æ—¶**ä¸ºä»€ä¹ˆéœ€è¦ pad & unpadã€‚

---

## ğŸŒŸ é—®é¢˜èƒŒæ™¯ï¼šä¸åŒç¯å¢ƒåœ¨ä¸åŒæ—¶é—´ doneï¼Œå†å²é•¿åº¦ä¸ä¸€è‡´

å‡è®¾ä½ æœ‰ 3 ä¸ªå¹¶è¡Œç¯å¢ƒï¼ˆenv0 / env1 / env2ï¼‰ï¼Œä½ æƒ³ç»™ RNN è¾“å…¥æœ€è¿‘ **3 æ­¥è§‚æµ‹å†å²**ï¼š

```
history_length = 3   # T = 3
```

çœŸå®æƒ…å†µå¯èƒ½æ˜¯ï¼š

| time step | env0 | env1   | env2                 |
| --------- | ---- | ------ | -------------------- |
| t-2       | æœ‰è§‚æµ‹  | æœ‰è§‚æµ‹    | âœ… done (episode é‡å¯äº†) |
| t-1       | æœ‰è§‚æµ‹  | âœ… done | âœ… done               |
| t         | æœ‰è§‚æµ‹  | æœ‰è§‚æµ‹    | æœ‰è§‚æµ‹                  |

> ä¹Ÿå°±æ˜¯è¯´ï¼Œ**æŸäº›ç¯å¢ƒçš„å†å²ä¸å¤Ÿé•¿**ï¼ˆå› ä¸ºä¸­é—´ doneï¼‰ï¼Œå¯¼è‡´å†å²æ•°æ®ç¼ºå¤±ã€‚

ä½† **RNN è¦æ±‚è¾“å…¥ shape å¿…é¡»ä¸¥æ ¼ä¸€è‡´ï¼š**

```
(T, num_envs, obs_dim)
```

å³ä½¿ env1ã€env2 çš„å†å²ç¼ºå¤±ï¼Œä¹Ÿå¿…é¡»é€è¿›å»ä¸€ä¸ªåŒ shape çš„ tensorã€‚

---

## âœ… pad å°±æ˜¯ç”¨æ¥â€œè¡¥å†å²â€çš„

å¯¹ç¼ºå¤±çš„æ—¶é—´æ­¥ï¼Œç”¨æŸä¸ªå€¼ï¼ˆé€šå¸¸æ˜¯ 0ï¼‰å¡«æ»¡ï¼š

```
åŸå§‹å†å² (æœª pad)ï¼š
env0: [o(t-2), o(t-1), o(t)]
env1: [o(t-2), â€”â€”, o(t)]
env2: [â€”â€”, â€”â€”, o(t)]
```

pad åå¾—åˆ°ç»Ÿä¸€ shape (3, 3, D) çš„ tensorï¼š

```
       t-2        t-1        t
-------------------------------------------------
env0 | o0(t-2)    o0(t-1)    o0(t)
env1 | o1(t-2)    PAD        o1(t)
env2 | PAD        PAD        o2(t)
```

ç”¨ç¤ºæ„å›¾ï¼š

```
Before pad (ragged):
[
  env0: [A, B, C]
  env1: [D,   , F]
  env2: [  ,   , G]
]

After pad:
[
  [A, D, PAD],   # t-2
  [B, PAD, PAD], # t-1
  [C, F, G],     # t
]
shape => (T=3, num_envs=3, obs_dim)
```

---

## âœ… mask è¡¨ç¤ºå“ªäº›æ˜¯ padï¼Œå“ªäº›æ˜¯æœ‰æ•ˆ

åŒæ—¶ç”Ÿæˆä¸€ä¸ª maskï¼š

```
mask: 1 è¡¨ç¤ºæœ‰æ•ˆï¼Œ0 è¡¨ç¤º pad

mask = [
  [1, 1, 0],    # t-2
  [1, 0, 0],    # t-1
  [1, 1, 1],    # t
]
```

---

## ğŸ¯ RNN å¦‚ä½•ç”¨ pad + maskï¼Ÿ

RNN ä¼šï¼š

âœ” å¯¹ mask = 0 çš„ time-step **ä¸æ›´æ–° hidden state**ï¼ˆé˜²æ­¢åä¿¡æ¯æ±¡æŸ“ RNNï¼‰
âœ” å¯¹ mask = 1 çš„ time-step æ­£å¸¸æ›´æ–°

å› æ­¤ env1 å’Œ env2 çš„å†å²ç¼ºå¤±**ä¸ä¼šè®© RNN è¯¯ä»¥ä¸ºå®ƒä»¬æœ‰è¿ç»­å†å²**ã€‚

---

## âœ… unpad çš„ä½œç”¨ï¼ˆè¿˜åŸå›å•æ­¥è¾“å‡ºï¼‰

åœ¨ä½ çš„æ¨¡å‹é‡Œçœ‹åˆ°ï¼š

```python
height_feat = unpad_trajectories(encoded, masks)
```

æ„æ€æ˜¯ï¼š

> è®­ç»ƒ RNN ç”¨ pad çš„åºåˆ—ï¼Œ
> ä½†æœ€åæˆ‘ä»¬åªéœ€è¦ **æ¯ä¸ª env çš„æœ€åæœ‰æ•ˆæ—¶é—´æ­¥** çš„ featureã€‚

ä½¿ç”¨ mask å»æ‰ pad åè¾“å‡ºï¼š

```
unpad ç»“æœï¼š
env0 -> C
env1 -> F
env2 -> G
```

shapeï¼š

```
(B, feature_dim)
```

---

## ğŸš€ æ€»ç»“ï¼ˆä¸€å¥è¯ï¼‰

| æ“ä½œ        | ä½œç”¨                                                          |
| --------- | ----------------------------------------------------------- |
| **pad**   | è®© ragged history å˜ä¸ºç»Ÿä¸€ tensor `(T, num_envs, D)`ï¼Œæ–¹ä¾¿ RNN æ‰¹é‡å¤„ç† |
| **mask**  | æ ‡è®°å“ªäº› timestep æ˜¯çœŸå®çš„ã€å“ªäº›æ˜¯ padï¼ˆé¿å…æ±¡æŸ“ RNNï¼‰                        |
| **unpad** | ä» RNN è¾“å‡ºä¸­åªä¿ç•™ **æœ€è¿‘æœ‰æ•ˆçš„ä¸€æ­¥**ï¼ˆä¸€èˆ¬ä½œä¸º featureï¼‰                      |

---

## ğŸ“Œ å°å®éªŒï¼ˆå¯ç›´æ¥è¿è¡ŒéªŒè¯ï¼‰

```python
import torch

obs_dim = 4
T = 3
B = 3

padded = torch.tensor([
    [[1,1,1,1], [2,2,2,2], [0,0,0,0]], # t-2
    [[3,3,3,3], [0,0,0,0], [0,0,0,0]], # t-1
    [[4,4,4,4], [5,5,5,5], [6,6,6,6]], # t
])

masks = torch.tensor([
    [1,1,0],
    [1,0,0],
    [1,1,1]
])

# ä¿ç•™æœ€åæœ‰æ•ˆ
result = []
for env in range(B):
    valid = padded[:, env][masks[:,env]==1]
    result.append(valid[-1])

result = torch.stack(result)
print(result)
```

è¾“å‡ºï¼š

```
tensor([[4,4,4,4],
        [5,5,5,5],
        [6,6,6,6]])
```

è¿™å°±æ˜¯ unpad çš„ä½œç”¨ã€‚

---

prepare_featuresé‡Œçš„observationæ˜¯ç»è¿‡paddingçš„å—

# è§‚å¯Ÿä¸extras
å…ˆç»™ä½ ä¸€ä¸ªç»“è®ºç‰ˆï¼Œç„¶åå†ä¸€ç‚¹ç‚¹æ‹†å¼€ğŸ‘‡

---

## 1. `obs, extras = self.env.get_observations()` é‡Œçš„ `extras` åˆ°åº•æ˜¯å•¥ï¼Ÿ

åœ¨ **IsaacLab + RSL-RL** çš„å°è£…é‡Œï¼š

* `obs`ï¼šç»™ **â€œä¸»ç­–ç•¥â€/å­¦ç”Ÿ (policy)** ç”¨çš„è§‚æµ‹ï¼ˆé€šå¸¸å°±æ˜¯ `ObservationsCfg.PolicyCfg` è¿™ç»„ï¼Œè¢«æ‹¼æˆä¸€ä¸ª `(num_envs, obs_dim)` çš„ tensorï¼‰ã€‚
* `extras`ï¼šä¸€ä¸ª **å­—å…¸**ï¼Œå­˜æ”¾â€œä¸æ˜¯ä¸»è§‚æµ‹â€çš„å„ç§ä¸œè¥¿ï¼ŒåŒ…æ‹¬ï¼š

  1. å…¶ä»– observation groupï¼ˆä¾‹å¦‚ä½ å®šä¹‰çš„ `critic` ç»„ = è€å¸ˆ/privileged obsï¼‰
  2. å„ç§ç¯å¢ƒå†…éƒ¨è®°å½•çš„é¢å¤–ä¿¡æ¯ï¼ˆå¦‚ episode ç»Ÿè®¡ã€log ä¿¡æ¯ã€time_outs ç­‰ï¼Œå…·ä½“ç”± env / wrapper å†³å®šï¼‰

RSL-RL çš„ `RslRlVecEnvWrapper` é‡Œæœ‰ä¸€æ®µé€»è¾‘ï¼ˆä¼ªä»£ç ï¼‰å¤§æ¦‚æ˜¯è¿™æ ·çš„ï¼ˆä½ å¯ä»¥æŠŠè¿™ä¸ªå½“ mental modelï¼‰ï¼š

```python
obs_dict = env.obs_buf  # è¿™é‡Œæ˜¯ä¸€ä¸ª {group_name: tensor} çš„ dict

# é€‰å‡ºä¸»è§‚æµ‹ï¼ˆä¸€èˆ¬æ˜¯ "policy" ç»„ï¼‰ï¼Œä½œä¸º RL ä¸­çš„ obs
policy_obs = obs_dict["policy"]

extras = {}
# å…¶ä½™ç»„ï¼ˆæ¯”å¦‚ "critic"ï¼‰è¢«è®¤ä¸ºæ˜¯ extra observationsï¼Œæ”¾è¿› extras
extra_obs_groups = {k: v for k, v in obs_dict.items() if k != "policy"}
if extra_obs_groups:
    extras["observations"] = extra_obs_groups

# æ­¤å¤–è¿˜ä¼šæŠŠ env.extras é‡Œçš„å†…å®¹åˆå¹¶è¿›æ¥ï¼ˆå¦‚ logã€time_outs ç­‰ï¼‰
extras.update(env.extras)

return policy_obs, extras
```

æ‰€ä»¥ï¼š**`extras` é‡Œæ—¢å¯èƒ½æœ‰ä½ çš„â€œè€å¸ˆè§‚æµ‹â€ï¼Œä¹Ÿå¯èƒ½æœ‰ logging / time_out æ ‡è®°ç­‰å…¶å®ƒä¸œè¥¿ã€‚**

---

## 2. å°±ä½ è¿™ä»½ `RobotEnvCfg`ï¼Œå“ªäº›ä¸œè¥¿ä¼šè¿›åˆ° `extras`ï¼Ÿ

çœ‹ä½ çš„ `ObservationsCfg`ï¼š

```python
@configclass
class ObservationsCfg:
    """Observation specifications for the MDP."""

    @configclass
    class PolicyCfg(ObsGroup):
        ...
        def __post_init__(self):
            self.enable_corruption = True
            self.concatenate_terms = True
    policy: PolicyCfg = PolicyCfg()

    @configclass
    class CriticCfg(ObsGroup):
        ...
    critic: CriticCfg = CriticCfg()
```

ä¹Ÿå°±æ˜¯è¯´ï¼š

* env å†…éƒ¨çš„ **observation_manager.compute()** ä¼šå¾—åˆ°ä¸€ä¸ª **å­—å…¸**ï¼š

  ```python
  {
      "policy": <tensor of shape (num_envs, policy_dim)>,
      "critic": <tensor of shape (num_envs, critic_dim)>
  }
  ```
* åœ¨ `ManagerBasedRLEnv.step()` / `get_observations()` è¿”å›ç»™ RSL-RL wrapper æ—¶ï¼š

  * **ä¸»è§‚æµ‹**ï¼šåªå– `"policy"` è¿™ä¸€ç»„ï¼ˆç»™ actor / å­¦ç”Ÿï¼‰
  * å…¶å®ƒç»„ï¼ˆè¿™é‡Œåªæœ‰ `"critic"`ï¼‰ä¼šè¢« wrapper è§†ä¸º â€œextra observationsâ€ï¼Œæ”¾è¿› `extras`ï¼Œå½¢å¼å¤§æ¦‚ç±»ä¼¼ï¼š

    ```python
    extras = {
        "observations": {
            "critic": critic_obs_tensor,  # (num_envs, critic_dim)
        },
        # è¿˜å¯èƒ½æœ‰ "log" / "time_outs" ç­‰å…¶å®ƒé”®
    }
    ```

å†åŠ ä¸Š `ManagerBasedEnv` / `ManagerBasedRLEnv` è‡ªå·±ç»´æŠ¤çš„ `self.extras` å­—å…¸ï¼Œé‡Œé¢é€šå¸¸ä¼šåœ¨ï¼š

* reset / step æ—¶è¢«å„ä¸ª manager å†™ä¸€äº›ä¿¡æ¯ï¼š

  * ä¾‹å¦‚ `reward_manager` å†™ episode return
  * `termination_manager` å†™å“ªäº› env è§¦å‘äº†å“ªç±»ç»ˆæ­¢
  * ä½ æ‰‹åŠ¨å¾€é‡Œå¡çš„ä¸€äº› debug metric

RSL-RL çš„ wrapper åœ¨ `step()` æ—¶è¿˜ä¼šæŠŠ `truncated`ï¼ˆtime_outï¼‰ä¹Ÿæ”¾åˆ° `extras["time_outs"]` é‡Œï¼ˆç”¨äºæ— é™/æœ‰é™ horizon çš„åŒºåˆ†ï¼‰ã€‚([docs.robotsfan.com][1])

**ç®€å•è®°å¿†ï¼š**

> å¯¹ä½ è¿™ä¸ª env æ¥è¯´ï¼Œ`obs` â‰ˆ policy ç»„ï¼ˆå­¦ç”Ÿï¼‰ï¼Œ
> `extras["observations"]["critic"]` â‰ˆ critic ç»„ï¼ˆè€å¸ˆ + privileged obsï¼‰ï¼Œ
> å…¶å®ƒè¯¸å¦‚ `extras["time_outs"]`ã€`extras["log"]` æ˜¯ç¯å¢ƒçš„é™„åŠ ä¿¡æ¯ã€‚

ä½ å¯ä»¥åœ¨ä»£ç é‡Œç›´æ¥éªŒè¯ä¸€ä¸‹ï¼ˆå¼ºçƒˆå»ºè®®ï¼‰ï¼š

```python
obs, extras = self.env.get_observations()
print(type(obs), obs.shape)             # (num_envs, policy_dim)
print(extras.keys())                    # çœ‹çœ‹æœ‰å•¥é”®
print(extras.get("observations", {}).keys())  # åº”è¯¥æœ‰ "critic"
print(extras["observations"]["critic"].shape) # (num_envs, critic_dim)
```

---

## 3. æƒ³åˆ†å¼€å­¦ç”Ÿ / æ•™å¸ˆè§‚æµ‹ï¼Œåº”è¯¥æ€ä¹ˆåšï¼Ÿ

ä½ ç°åœ¨çš„é…ç½®å…¶å®å·²ç»æ˜¯ **ç»å…¸ student / teacherï¼ˆactor / critic privileged obsï¼‰å†™æ³•** äº†ï¼Œæ¥ä¸‹æ¥åªè¦åœ¨è®­ç»ƒä»£ç é‡Œæ­£ç¡®å–å°±è¡Œã€‚

### 3.1 â€œå­¦ç”Ÿ / æ•™å¸ˆâ€åœ¨è¿™ä¸ªé…ç½®é‡Œçš„å¯¹åº”å…³ç³»

* å­¦ç”Ÿï¼ˆpolicy / actorï¼‰ï¼šç”¨ `ObservationsCfg.PolicyCfg` å¯¹åº”çš„è§‚æµ‹

  * ä½ å·²ç»æŠŠ `PolicyCfg.enable_corruption = True`ï¼Œå¯ä»¥åœ¨è¿™é‡Œåšå™ªå£° / ä¸å®Œå…¨è§‚æµ‹ç­‰å¤„ç†ï¼Œé€‚åˆä½œä¸º **å­¦ç”Ÿè§‚æµ‹**ã€‚
* è€å¸ˆï¼ˆcritic / privilegedï¼‰ï¼šç”¨ `ObservationsCfg.CriticCfg` å¯¹åº”çš„è§‚æµ‹

  * å¯ä»¥åŒ…å«æ›´å¤šçš„ã€ç”šè‡³æ˜¯â€œä½œå¼Šâ€çš„ä¿¡æ¯ï¼ˆä¾‹å¦‚çœŸå®é€Ÿåº¦ã€trajã€é«˜åº¦å›¾ç­‰ï¼‰ï¼Œä¸å¯¹çœŸå®æœºå™¨äººæš´éœ²ï¼Œåªç»™ critic / teacher æ¨¡å—ç”¨ã€‚

è¿™å°±æ˜¯ IsaacLab å®˜æ–¹ legged ç¤ºä¾‹é»˜è®¤é‡‡ç”¨çš„â€œå­¦ç”Ÿ / è€å¸ˆåˆ†è§‚æµ‹â€çš„å¥—è·¯ã€‚

### 3.2 åœ¨è®­ç»ƒä»£ç ä¸­æ€ä¹ˆå–ï¼Ÿ

#### æƒ…å½¢ Aï¼šä½ ç”¨çš„æ˜¯ RSL-RL + `RslRlVecEnvWrapper`

å…¸å‹ step / get_observations å†™æ³•ï¼š

```python
# 1) å–è§‚æµ‹
obs, extras = env.get_observations()   # obs: å­¦ç”Ÿ; extras: é‡Œé¢è—ç€è€å¸ˆ
student_obs = obs                      # shape: (num_envs, policy_dim)

# 2) ä» extras é‡Œå–è€å¸ˆï¼ˆcriticï¼‰è§‚æµ‹
teacher_obs = None
if "observations" in extras and "critic" in extras["observations"]:
    teacher_obs = extras["observations"]["critic"]  # shape: (num_envs, critic_dim)

# 3) ä½ å°±å¯ä»¥ï¼š
# - ç”¨ student_obs å–‚ actor ç½‘ç»œ
# - ç”¨ teacher_obs å–‚ critic ç½‘ç»œ / teacher policy / AMP discriminator ç­‰
```

å¦‚æœä½ è‡ªå·±å†™ runnerï¼Œè€Œä¸æ˜¯ç”¨å®˜æ–¹ PPO runnerï¼Œå¯ä»¥æ˜¾å¼æŠŠ `teacher_obs` ä¼ ç»™ criticï¼š

```python
value = critic(student_obs, teacher_obs)  # æˆ–è€…åªä¼  teacher_obsï¼Œçœ‹ä½ æ€ä¹ˆè®¾è®¡
```

#### æƒ…å½¢ Bï¼šä½ ä¸æƒ³é€šè¿‡ `extras`ï¼Œæƒ³è‡ªå·±æ‰‹åŠ¨æ§åˆ¶

ä½ ä¹Ÿå¯ä»¥ç›´æ¥è°ƒç”¨ env çš„ observation_managerï¼š

```python
# ä¸é€šè¿‡ wrapperï¼Œç›´æ¥é—® env å½“å‰æ‰€æœ‰è§‚æµ‹
obs_dict = env.unwrapped.observation_manager.compute(update_history=False)

student_obs = obs_dict["policy"]   # (num_envs, policy_dim)
teacher_obs = obs_dict["critic"]   # (num_envs, critic_dim)
```

è¿™æ ·å°±å®Œå…¨è‡ªå·±æŒæ§ï¼Œä¸ä¾èµ– wrapper å¸®ä½ å¡è¿› `extras`ã€‚

---

## 4. å¦‚æœæƒ³æ›´æ˜ç¡®åœ°åŒºåˆ†å¹¶ debug å­¦ç”Ÿ / è€å¸ˆè§‚æµ‹ï¼Ÿ

ç»™ä½ å‡ ä¸ªå®ç”¨æ“ä½œï¼š

### 4.1 æ‰“å° group ä¿¡æ¯å’Œ shape

```python
obs_mgr = env.unwrapped.observation_manager
print("groups:", obs_mgr.active_terms.keys())
for group_name in obs_mgr.active_terms:
    print(group_name,
          "dim =", obs_mgr.group_obs_dim[group_name],
          "concat =", obs_mgr.group_obs_concatenate[group_name])
```

ä½ ä¼šçœ‹åˆ°ç±»ä¼¼ï¼š

* `policy dim = (N,) concat = True`
* `critic dim = (M,) concat = True`

### 4.2 åœ¨ä¸€æ­¥è®­ç»ƒé‡Œå®é™… dump ä¸€ä¸‹

```python
obs, extras = env.get_observations()
print("student_obs:", obs.shape)
if "observations" in extras:
    for k, v in extras["observations"].items():
        print(f"extra obs group {k}: {v.shape}")
print("extras keys:", extras.keys())
```

è·‘å‡ æ­¥åï¼Œä½ å°±éå¸¸ç›´è§‚åœ°çŸ¥é“ï¼š

* å­¦ç”Ÿ obs æ˜¯ä»€ä¹ˆ shape
* è€å¸ˆ/critic obs æ˜¯åœ¨å“ªé‡Œã€æ˜¯ä»€ä¹ˆ shape
* `extras` é‡Œè¿˜æœ‰å“ªäº›ä¿¡æ¯å¯ä»¥åˆ©ç”¨ï¼ˆå¦‚ episode ç»Ÿè®¡ã€time_outsï¼‰

---

## 5. ä¸€å¥è¯æ€»ç»“

* **`extras` = â€œé™¤äº†ä¸»è§‚æµ‹ä»¥å¤–ï¼Œæˆ‘è¿™ä¸ª vectorized env æƒ³é¡ºå¸¦å‘Šè¯‰ä½ çš„æ‰€æœ‰ä¸œè¥¿â€çš„åƒåœ¾æ¡¶**â€”â€”åŒ…æ‹¬åˆ«çš„ obs ç»„ï¼ˆæ¯”å¦‚ `critic`/è€å¸ˆï¼‰ã€logã€time_outs ç­‰ã€‚
* ä½ çš„é…ç½®å·²ç»å¤©ç„¶æŠŠ **å­¦ç”Ÿ** æ˜ å°„åˆ° `policy` ç»„ï¼ŒæŠŠ **è€å¸ˆ / privileged** æ˜ å°„åˆ° `critic` ç»„ã€‚
* ç”¨ `obs, extras = env.get_observations()` æ—¶ï¼š

  * `obs` ç»™å­¦ç”Ÿ
  * ä» `extras["observations"]["critic"]` æ‹¿è€å¸ˆï¼Œæˆ–è€…ç›´æ¥ç”¨ `observation_manager.compute()` è‡ªå·±å–ã€‚

å¦‚æœä½ æŠŠä½ ç°åœ¨çš„è®­ç»ƒè„šæœ¬ core loop è´´ä¸€æ®µå‡ºæ¥ï¼Œæˆ‘è¿˜èƒ½å¸®ä½ æŠŠâ€œå­¦ç”Ÿ/æ•™å¸ˆè§‚æµ‹æµå‘â€ç”»æˆä¸€ä¸ªç®€æ´çš„æ•°æ®æµå›¾ï¼Œä½ ä»¥åçœ‹å°±ä¸€ç›®äº†ç„¶ ğŸ‘€

[1]: https://docs.robotsfan.com/isaaclab/_modules/isaaclab_rl/rsl_rl/vecenv_wrapper.html?utm_source=chatgpt.com "isaaclab_rl.rsl_rl.vecenv_wrapper â€” Isaac Lab æ–‡æ¡£"


# stage2todo
- moduleä¸­æ–°çš„student_teacher_recurrent
- module: discriminator
- algorithm: AEPçš„è®­ç»ƒæ–¹å¼ï¼šå¯¹discriminorçš„è®­ç»ƒ + RLéƒ¨åˆ†

# AMP-rsl_rl
æˆ‘å…ˆæŒ‰â€œç›‘ç£ä¿¡å· / loss ç»“æ„ / minibatch_std_scalarâ€ä¸‰ä¸ªå—æ¥è®²ï¼Œè¿™æ ·ä½ å¯ä»¥ç›´æ¥å¯¹ç…§ä»£ç ç†è§£ AMP çš„åˆ¤åˆ«å™¨åœ¨å¹²å˜›ã€‚

---

## 1. åˆ¤åˆ«å™¨è¾“å…¥ & ç›‘ç£ä¿¡å·åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ

### åˆ¤åˆ«å™¨è¾“å…¥æ˜¯ä»€ä¹ˆï¼Ÿ

åœ¨ AMP é‡Œï¼Œåˆ¤åˆ«å™¨ D çš„è¾“å…¥æ˜¯

> å½“å‰ state å’Œ next_state çš„æ‹¼æ¥ï¼š`x = cat([state, next_state], dim=-1)`

ä¹Ÿå°±æ˜¯ä¸€æ¡ â€œtransitionâ€ï¼ˆæˆ–è€…è¯´ä¸€ä¸ªç‰‡æ®µçš„å±€éƒ¨ï¼‰ï¼Œæ—¢æœ‰å½“å‰çŠ¶æ€ï¼Œåˆæœ‰ä¸‹ä¸€æ­¥çŠ¶æ€ã€‚

* å¯¹ **expert**ï¼š`(state_expert, next_state_expert)`
* å¯¹ **policy**ï¼š`(state_policy, next_state_policy)`

éƒ½è¢«æ‹¼æˆä¸€ä¸ªå‘é‡ï¼Œç„¶åä¸¢è¿› `Discriminator.forward(x)`ï¼š

```python
def forward(self, x):
    h = self.trunk(x)        # å¤šå±‚ MLP
    if self.use_minibatch_std:
        s = self._minibatch_std_scalar(h)
        h = torch.cat([h, s], dim=-1)    # ç»™æ¯ä¸ªæ ·æœ¬æ‹¼ä¸€ä¸ªé¢å¤–çš„ 1 ç»´ç‰¹å¾
    return self.linear(h)    # è¾“å‡º shape: (B, 1) çš„ logit / score
```

**ç›‘ç£ä¿¡å·ï¼ˆæ ‡ç­¾ï¼‰éå¸¸ç®€å•ï¼š**

* Expert è½¨è¿¹ â†’ label = 1
* Policy è½¨è¿¹ â†’ label = 0

ä¹Ÿå°±æ˜¯â€œä½ æ˜¯äººç±»ç¤ºèŒƒè¿˜æ˜¯ RL ç­–ç•¥äº§ç”Ÿçš„ï¼Ÿâ€

---

## 2. BCE / WGAN ä¸¤ç§ loss æ˜¯æ€ä¹ˆç”¨çš„ï¼Ÿ

### 2.1 `policy_loss` / `expert_loss` è¿™ä¸¤ä¸ªå‡½æ•°åœ¨å¹²å˜›ï¼Ÿ

```python
def policy_loss(self, discriminator_output):
    expected = torch.zeros_like(discriminator_output, device=self.device)
    return self.loss_fun(discriminator_output, expected)

def expert_loss(self, discriminator_output):
    expected = torch.ones_like(discriminator_output, device=self.device)
    return self.loss_fun(discriminator_output, expected)
```

è¿™é‡Œçš„ `self.loss_fun` åœ¨ `__init__` é‡Œè¢«è®¾ä¸ºï¼š

```python
if self.loss_type == "BCEWithLogits":
    self.loss_fun = torch.nn.BCEWithLogitsLoss()
```

ä¹Ÿå°±æ˜¯è¯´ï¼š

* `discriminator_output` æ˜¯ **logit**ï¼ˆè¿˜æ²¡è¿‡ sigmoid çš„ï¼‰
* `expected` æ˜¯æ ‡ç­¾ï¼ˆ0 æˆ– 1ï¼‰
* `BCEWithLogitsLoss(logit, y)` = äº¤å‰ç†µï¼š
  [
  \text{BCEWithLogits}(z,y) = -\big[ y\log \sigma(z) + (1-y)\log(1-\sigma(z))\big]
  ]

æ‰€ä»¥ï¼š

* `policy_loss`ï¼šå¸Œæœ› policy æ ·æœ¬çš„åˆ¤åˆ«ç»“æœè¶‹è¿‘äº 0ï¼ˆâ€œå‡çš„â€ï¼‰
* `expert_loss`ï¼šå¸Œæœ› expert æ ·æœ¬çš„åˆ¤åˆ«ç»“æœè¶‹è¿‘äº 1ï¼ˆâ€œçœŸçš„â€ï¼‰

è¿™å°±æ˜¯æœ€æ™®é€šçš„ GAN åˆ¤åˆ«å™¨ç›‘ç£ä¿¡å·ã€‚

> æ³¨æ„ï¼šä¸‹é¢ `compute_loss` é‡Œå…¶å®åˆè‡ªå·±å†™äº†ä¸€é expert/policy çš„ BCEï¼Œä¸æ˜¯ç›´æ¥è°ƒç”¨ä¸Šé¢ä¸¤ä¸ª helper å‡½æ•°ï¼Œæœ¬è´¨æ˜¯ä¸€æ ·çš„ã€‚

---

### 2.2 `compute_loss`ï¼šçœŸæ­£è®­ç»ƒåˆ¤åˆ«å™¨æ—¶ç”¨çš„æ˜¯è°ï¼Ÿ

```python
def compute_loss(
    self,
    policy_d,
    expert_d,
    sample_amp_expert,
    sample_amp_policy,
    lambda_: float = 10,
):
    # 1) å…ˆç®— gradient penalty
    grad_pen_loss = self.compute_grad_pen(
        expert_states=sample_amp_expert,
        policy_states=sample_amp_policy,
        lambda_=lambda_,
    )

    if self.loss_type == "BCEWithLogits":
        expert_loss = self.loss_fun(expert_d, torch.ones_like(expert_d))
        policy_loss = self.loss_fun(policy_d, torch.zeros_like(policy_d))
        # åˆ¤åˆ«å™¨çš„ AMP loss = expert_loss & policy_loss çš„å¹³å‡
        amp_loss = 0.5 * (expert_loss + policy_loss)

    elif self.loss_type == "Wasserstein":
        amp_loss = self.wgan_loss(policy_d=policy_d, expert_d=expert_d)

    return amp_loss, grad_pen_loss
```

è¿™é‡Œä¼ è¿›æ¥çš„ï¼š

* `expert_d` = `D(expert_state, expert_next_state)` çš„è¾“å‡º logits / scores
* `policy_d` = `D(policy_state, policy_next_state)` çš„è¾“å‡º logits / scores
* `sample_amp_expert` / `sample_amp_policy` åˆ™æ˜¯ `(state, next_state)` çš„ tupleï¼Œç”¨äº gradient penalty

#### 2.2.1 BCEWithLogits æ¨¡å¼

* åˆ¤åˆ«å™¨ lossï¼š
  [
  L_{\text{disc}} = \frac{1}{2} \Big( \text{BCE}(D(x_\text{expert}), 1) + \text{BCE}(D(x_\text{policy}), 0) \Big)
  ]

* Gradient penaltyï¼šä¸‹é¢è¯¦ç»†è®² `compute_grad_pen`ã€‚

* æ€»åˆ¤åˆ«å™¨ lossï¼ˆåœ¨å¤–é¢ trainer é‡Œï¼‰ï¼šé€šå¸¸æ˜¯
  [
  L_{\text{total}} = L_{\text{disc}} + L_{\text{grad-pen}}
  ]

#### 2.2.2 Wasserstein æ¨¡å¼

å¦‚æœ `loss_type == "Wasserstein"`ï¼š

```python
def wgan_loss(self, policy_d, expert_d):
    policy_d = torch.tanh(self.eta_wgan * policy_d)
    expert_d = torch.tanh(self.eta_wgan * expert_d)
    return policy_d.mean() - expert_d.mean()
```

* åŸå§‹ WGAN loss é€šå¸¸æ˜¯ï¼š
  [
  L = \mathbb{E}[D(\text{fake})] - \mathbb{E}[D(\text{real})]
  ]
  åˆ¤åˆ«å™¨å¸Œæœ› **å‡å°**è¿™ä¸ªå€¼ï¼Œä¹Ÿå°±æ˜¯è®© `D(real)` å¤§ã€`D(fake)` å°ã€‚

* è¿™é‡Œåšäº†ä¸€ä¸ªæ”¹åŠ¨ï¼šå…ˆå¯¹è¾“å‡ºä¹˜ä»¥ `eta_wgan` å†è¿‡ `tanh` åšå‹ç¼©ï¼š

  * é˜²æ­¢åˆ¤åˆ«å™¨è¾“å‡ºå‘æ•£å¤ªå¤§ï¼ˆè®­ç»ƒä¸ç¨³å®šï¼‰
  * ä¿æŒ sign å’Œç›¸å¯¹å¤§å°ï¼Œä½¿å¾—â€œreal > fakeâ€ä»ç„¶æˆç«‹

é…å¥—çš„ `compute_grad_pen` åœ¨ Wasserstein æ¨¡å¼ä¸‹å°±æ˜¯ **WGAN-GP**ï¼š

```python
if self.loss_type == "Wasserstein":
    policy = torch.cat(policy_states, -1)
    alpha = torch.rand(expert.size(0), 1, device=expert.device)
    alpha = alpha.expand_as(expert)
    data = alpha * expert + (1 - alpha) * policy   # ä¸€æ¡æ’å€¼çº¿ä¸Šçš„ç‚¹
    data = data.detach().requires_grad_(True)
    h = self.trunk(data)
    if self.use_minibatch_std:
        with torch.no_grad():
            s = self._minibatch_std_scalar(h)
        h = torch.cat([h, s], dim=-1)
    scores = self.linear(h)
    grad = autograd.grad(
        outputs=scores,
        inputs=data,
        grad_outputs=torch.ones_like(scores),
        create_graph=True,
        retain_graph=True,
        only_inputs=True,
    )[0]
    return lambda_ * (grad.norm(2, dim=1) - 1.0).pow(2).mean()
```

* åœ¨ expert & policy ä¹‹é—´åšæ’å€¼ï¼š
  [
  x_\text{interp} = \alpha x_\text{expert} + (1-\alpha)x_\text{policy}
  ]
* ç®— `âˆ¥âˆ‡_x D(x_interp)âˆ¥_2`ï¼Œåšï¼š
  [
  L_{\text{GP}} = \lambda (\lVert \nabla_x D(x)\rVert_2 - 1)^2
  ]
  è¿™æ˜¯ WGAN-GP é‡Œ enforce 1-Lipschitz çš„ç»å…¸æ–¹æ³•ã€‚

#### 2.2.3 BCE æ¨¡å¼ä¸‹çš„ `compute_grad_pen` æ˜¯ä»€ä¹ˆï¼Ÿ

```python
elif self.loss_type == "BCEWithLogits":
    # R1 regularizer on REAL: 0.5 * lambda * ||âˆ‡_x D(x_real)||^2
    data = expert.detach().requires_grad_(True)
    h = self.trunk(data)
    if self.use_minibatch_std:
        with torch.no_grad():
            s = self._minibatch_std_scalar(h)
        h = torch.cat([h, s], dim=-1)
    scores = self.linear(h)

    grad = autograd.grad(
        outputs=scores.sum(),
        inputs=data,
        create_graph=True,
        retain_graph=True,
        only_inputs=True,
    )[0]
    return 0.5 * lambda_ * (grad.pow(2).sum(dim=1)).mean()
```

è¿™æ˜¯ **R1 regularizer**ï¼ˆMescheder ç­‰äººæå‡ºçš„ï¼‰ï¼š

[
L_{\text{R1}} = \frac{\lambda}{2} \mathbb{E}*{x \sim p*\text{real}} \left[\lVert \nabla_x D(x)\rVert_2^2\right]
]

ä½œç”¨ï¼š

* æƒ©ç½š D å¯¹çœŸå®æ•°æ®é™„è¿‘çš„æ¢¯åº¦è¿‡å¤§
* å¹³æ»‘åˆ¤åˆ«å™¨ï¼ŒæŠ‘åˆ¶æ¢¯åº¦çˆ†ç‚¸ & æ¨¡å¼å´©å¡Œ
* å¯¹äºäºŒåˆ†ç±» BCE å½¢å¼çš„ GANï¼ŒR1 æ˜¯éå¸¸å¸¸è§çš„æ­£åˆ™é¡¹

> æ³¨æ„è¿™é‡Œè®¡ç®— `scores.sum()` å†å¯¹ `data` æ±‚å¯¼ï¼Œæ˜¯å› ä¸ºæˆ‘ä»¬è¦å¯¹ batch ä¸­æ¯ä¸ªæ ·æœ¬çš„æ¢¯åº¦æ±‚å’Œå†å¹³å‡ï¼Œç›¸å½“äºå¯¹æ¯ä¸ªè¾“å‡ºéƒ½è®¡ç®—ä¸€æ¬¡æ¢¯åº¦ã€‚

---

## 3. `predict_reward`ï¼šç»™ policy çš„ â€œä¼ª rewardâ€ æ€ä¹ˆæ¥çš„ï¼Ÿ

```python
def predict_reward(self, state, next_state, normalizer=None):
    with torch.no_grad():
        if normalizer is not None:
            state = normalizer(state)
            next_state = normalizer(next_state)

        discriminator_logit = self.forward(torch.cat([state, next_state], dim=-1))

        if self.loss_type == "Wasserstein":
            discriminator_logit = torch.tanh(self.eta_wgan * discriminator_logit)
            return self.reward_scale * torch.exp(discriminator_logit).squeeze()

        # BCE æ¨¡å¼
        reward = F.softplus(discriminator_logit)
        reward = self.reward_scale * reward
        return reward.squeeze()
```

### 3.1 Wasserstein æ¨¡å¼

* å¾—åˆ° score `s = D(x)`ï¼ˆå†ç»è¿‡ `tanh(eta * s)` å‹ç¼©ï¼‰
* reward ä½¿ç”¨ï¼š
  [
  r = \text{reward_scale} \cdot e^{\tilde{s}}
  ]
  è¯„åˆ†è¶Šå¤§ï¼Œreward è¶Šå¤§ã€‚

å› ä¸º WGAN ä¸­ score æœ¬èº«å°±è¿‘ä¼¼ Wasserstein è·ç¦»çš„å·®ï¼Œ`exp` åšäº†ä¸€ä¸ªå•è°ƒæ”¾å¤§ã€‚

### 3.2 BCE æ¨¡å¼

* `F.softplus(logit)`ï¼š
  softplus(z) = log(1 + e^z)ï¼Œæœ‰ä¸€ä¸ªé‡è¦æ’ç­‰å¼ï¼š

  > `softplus(z) = -log(1 - sigmoid(z))`

  è®° (D(x) = \sigma(z))ï¼Œåˆ™

  [
  \text{softplus}(z) = -\log(1 - D(x))
  ]

* è¿™éå¸¸åƒ **GAIL** é‡Œçš„ rewardï¼š
  æ ‡å‡† GAIL reward å¸¸ç”¨ï¼š
  [
  r(x) = -\log(1 - D(x))
  \quad \text{æˆ–} \quad
  r(x) = \log D(x) - \log(1 - D(x))
  ]

* å«ä¹‰ï¼š

  * å¦‚æœåˆ¤åˆ«å™¨è®¤ä¸ºè¿™ä¸ªæ ·æœ¬â€œå¾ˆçœŸå®â€ï¼ˆD(x) æ¥è¿‘ 1ï¼‰ï¼Œåˆ™ reward å¾ˆå¤§ã€‚
  * å¦‚æœ D(x) æ¥è¿‘ 0ï¼Œreward æ¥è¿‘ 0ã€‚

å†ä¹˜ä¸Š `reward_scale` åšå…¨å±€ç¼©æ”¾ã€‚
è¿™å°±æ˜¯ AMP ç»™ policy çš„ **æ¨¡ä»¿å¥–åŠ±ä¿¡å·**ï¼ŒRL é‡Œçš„ç¯å¢ƒ reward ä¼šåŠ ä¸Šå®ƒã€‚

---

## 4. `_minibatch_std_scalar` åˆ°åº•åœ¨å¹²å˜›ï¼Ÿæœ‰ä»€ä¹ˆç”¨ï¼Ÿ

æ¥çœ‹ä»£ç ï¼š

```python
def _minibatch_std_scalar(self, h: torch.Tensor) -> torch.Tensor:
    """Mean over feature-wise std across the batch; shape (B,1)."""
    if h.shape[0] <= 1:
        return h.new_zeros((h.shape[0], 1))
    s = h.float().std(dim=0, unbiased=False).mean()
    return s.expand(h.shape[0], 1).to(h.dtype)
```

å‡è®¾ `h` çš„ shape æ˜¯ `(B, F)`ï¼š

1. `h.std(dim=0)`ï¼šå¯¹ **batch ç»´** æ±‚ **æ¯ä¸ªç‰¹å¾ç»´åº¦çš„æ ‡å‡†å·®**

   * å¾—åˆ° `(F,)`ï¼šæ¯ä¸€åˆ—ï¼ˆæ¯ä¸ª featureï¼‰åœ¨æ•´ä¸ª batch ä¸Šçš„æ³¢åŠ¨ç¨‹åº¦

2. `.mean()`ï¼šå†å¯¹è¿™ `F` ä¸ª std å–å‡å€¼

   * å¾—åˆ° **ä¸€ä¸ªæ ‡é‡ s**ï¼šæ•´ä¸ª batch çš„â€œå¹³å‡å¤šæ ·æ€§â€

3. `s.expand(h.shape[0], 1)`ï¼šæŠŠè¿™ä¸ªæ ‡é‡å¤åˆ¶æˆ `(B, 1)`ï¼Œç”¨äºç»™ batch ä¸­æ¯ä¸ªæ ·æœ¬åŠ ä¸ŠåŒä¸€ä¸ªé¢å¤–ç‰¹å¾ç»´åº¦

åœ¨ `forward` é‡Œä½¿ç”¨ï¼š

```python
h = self.trunk(x)               # (B, hidden_dim)
if self.use_minibatch_std:
    s = self._minibatch_std_scalar(h)   # (B, 1)
    h = torch.cat([h, s], dim=-1)       # (B, hidden_dim+1)
return self.linear(h)
```

### 4.1 ç›´è§‚ç†è§£ï¼šå®ƒä¸ºä»€ä¹ˆæœ‰ç”¨ï¼Ÿ

è¿™ä¸ª trick æ¥è‡ª PGGAN / StyleGAN çš„ç»å…¸è®¾è®¡ï¼Œå« **minibatch standard deviation**ï¼š

* åˆ¤åˆ«å™¨ä¸ä»…çœ‹ â€œå•ä¸ªæ ·æœ¬é•¿ä»€ä¹ˆæ ·â€ï¼Œè¿˜çœ‹ â€œè¿™ä¸€å°æ‰¹æ ·æœ¬æ•´ä½“å¤šæ ·æ€§æœ‰å¤šå¤§â€
* å¦‚æœç”Ÿæˆå™¨ collapseï¼ˆmode collapseï¼‰ï¼ŒåŒä¸€ä¸ª batch é‡Œå¾ˆå¤šæ ·æœ¬å‡ ä¹ä¸€æ · â†’ std éå¸¸å° â†’ s å¾ˆå°
* åˆ¤åˆ«å™¨å¯ä»¥å­¦åˆ°ï¼šä½å¤šæ ·æ€§çš„ batch æ›´å¯èƒ½æ˜¯ **fake**ï¼Œä»è€Œç»™è¿™ç±»å‡æ ·æœ¬æ‰“ä½åˆ† â†’ å¼ºè¿«ç”Ÿæˆå™¨å»å¢åŠ å¤šæ ·æ€§

åœ¨è¿™é‡Œçš„ AMP é‡Œä¹Ÿæ˜¯åŒç†ï¼š

* Expert è½¨è¿¹é‡‡æ ·é€šå¸¸æ¯”è¾ƒå¤šæ ·ï¼ˆå„ç§å§¿æ€ã€é€Ÿåº¦ã€ç¯å¢ƒï¼‰ï¼Œbatch ä¸­åˆ†å¸ƒè¾ƒå¹¿ â†’ `minibatch std` è¾ƒå¤§
* Policy åˆæœŸç”Ÿæˆçš„è½¨è¿¹æ¯”è¾ƒå•ä¸€ã€åƒµç¡¬ â†’ `minibatch std` è¾ƒå°
* åˆ¤åˆ«å™¨å¤šäº†ä¸€ç»´â€œæ•´ä½“å¤šæ ·æ€§ä¿¡æ¯â€ï¼Œèƒ½æ›´å®¹æ˜“åŒºåˆ† expert / policy

**æ³¨æ„ï¼š**

åœ¨ `compute_grad_pen` é‡Œä½¿ç”¨ `minibatch_std` æ—¶ï¼Œç‰¹æ„ç”¨ï¼š

```python
if self.use_minibatch_std:
    with torch.no_grad():
        s = self._minibatch_std_scalar(h)
    h = torch.cat([h, s], dim=-1)
```

ä¹Ÿå°±æ˜¯è¯´ **R1 / WGAN-GP çš„ gradient penalty ä¸å…è®¸é€šè¿‡ s å›ä¼ æ¢¯åº¦**ï¼Œåªå¯¹è¾“å…¥ `data` æœ¬èº«çš„æ¢¯åº¦è¿›è¡Œæ­£åˆ™åŒ–ã€‚
å¦åˆ™ï¼ŒD ä¼šâ€œå·æ‡’â€é€šè¿‡åŠ¨ minibatch ç»Ÿè®¡å»è§„é¿ Lipschitz æˆ– R1 çº¦æŸï¼Œä¼šç ´åæ­£åˆ™çš„æ„ä¹‰ã€‚

---

## 5. å°ç»“ï¼ˆå¸®ä½ åœ¨è„‘å­é‡Œå»ºç«‹æ•´ä½“å›¾ï¼‰

1. **è¾“å…¥**ï¼š`x = [state, next_state]`ï¼Œexpert æ¥çš„æ‰“ label=1ï¼Œpolicy æ¥çš„ label=0ã€‚
2. **åˆ¤åˆ«å™¨**ï¼šMLP(`trunk`) â†’ å¯é€‰ `minibatch_std_scalar`ï¼ˆ1 ç»´ batch å¤šæ ·æ€§ç‰¹å¾ï¼‰ â†’ `linear` å¾—åˆ° logit / scoreã€‚
3. **è®­ç»ƒåˆ¤åˆ«å™¨çš„ lossï¼š**

   * BCE æ¨¡å¼ï¼š

     * åˆ¤åˆ«å™¨åˆ†ç±» lossï¼š`0.5*(BCE(expert_d,1) + BCE(policy_d,0))`
     * æ­£åˆ™ï¼šR1 regularizer on real data (`0.5*lambda*||âˆ‡_x D(x_real)||^2`)
   * Wasserstein æ¨¡å¼ï¼š

     * åˆ¤åˆ«å™¨ lossï¼š`mean(tanh(eta*policy_d)) - mean(tanh(eta*expert_d))`
     * æ­£åˆ™ï¼šWGAN-GP æ’å€¼ gradient penalty
4. **ç»™ policy çš„ adversarial rewardï¼š**

   * Wasserstein æ¨¡å¼ï¼š`reward ~ exp(tanh(eta * D(x)))`
   * BCE æ¨¡å¼ï¼š`reward ~ softplus(logit) = -log(1 - D(x))`ï¼ˆGAIL é£æ ¼ï¼‰
5. **`_minibatch_std_scalar`ï¼š**

   * è®¡ç®—â€œè¯¥ batch åœ¨ feature ç©ºé—´çš„å¹³å‡æ ‡å‡†å·®â€ï¼Œä½œä¸ºä¸€ç»´â€œå¤šæ ·æ€§ç‰¹å¾â€
   * ç›®çš„æ˜¯è®©åˆ¤åˆ«å™¨èƒ½å‘ç° **batch å¤šæ ·æ€§å·®å¼‚**ï¼Œæƒ©ç½š mode collapseï¼Œæå‡åŒºåˆ†åº¦
   * åœ¨ gradient penalty ä¸­å¯¹å®ƒ `detach`ï¼Œé¿å…ç ´å Lipschitz / R1 çº¦æŸçš„æ„ä¹‰

---

å¦‚æœä½ æ„¿æ„ï¼Œä¸‹ä¸€æ­¥æˆ‘å¯ä»¥å¸®ä½ ï¼š

* ç”»ä¸€ä¸ªç®€å•çš„æ¡†å›¾ï¼ˆæ–‡æœ¬ç‰ˆï¼‰è¯´æ˜ â€œexpert/policy â†’ D â†’ loss/rewardâ€çš„æ•°æ®æµã€‚
* æˆ–è€…ç›´æ¥ç»“åˆä½  SEAMP / AMP çš„è®­ç»ƒ loopï¼Œå¸®ä½ æ ‡å‡ºå“ªé‡Œè°ƒç”¨ `compute_loss`ã€å“ªé‡Œç”¨ `predict_reward`ï¼Œè¿™æ ·ä½ æ›´å®¹æ˜“åœ¨è‡ªå·±çš„é¡¹ç›®ä¸­ debugã€‚

# save ä¸ load
æ–°è’¸é¦æ—¶ï¼ŒåŠ è½½æ•™å¸ˆæ¨¡å‹ï¼Œæ­¤æ—¶ä¼šè¯†åˆ«è¿™ä¸ªpolicyæ˜¯acæ¶æ„çš„è¿˜æ˜¯tsæ¶æ„ï¼Œæ¥åˆ¤æ–­æ˜¯ä¸æ˜¯æ•™å¸ˆ
åœ¨è®­ç»ƒæ—¶ï¼Œä¿å­˜ckptæ—¶ï¼Œä¼šå°†è¯¥tsï¼ˆç»§æ‰¿è‡ªnn.moduleï¼‰ï¼Œä¿å­˜ä¸‹æ¥ï¼š

self.alg.policy æ˜¯ä¸€ä¸ª nn.Moduleï¼ˆå¦‚ ActorCritic æˆ– TerrainAwareStudentTeacherï¼‰ã€‚
è°ƒç”¨ self.alg.policy.state_dict() ä¼šè¿”å›ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯å±‚åï¼ˆå±‚çº§å¼å‰ç¼€ï¼‰ï¼Œå€¼æ˜¯å¼ é‡ï¼Œä¾‹å¦‚ï¼š
teacher.*ï¼ˆæ•™å¸ˆç½‘ç»œçš„ç¼–ç å™¨/actor/criticæƒé‡ä¸ç¼“å†²ï¼‰
memory_s.*ï¼ˆå­¦ç”Ÿ RNN/LSTM çš„æƒé‡ä¸çŠ¶æ€å½¢çŠ¶ç›¸å…³ç¼“å†²ï¼‰
student.encoder.*ï¼ˆå­¦ç”Ÿ MLP ç¼–ç å™¨ï¼‰
student.policy.*ï¼ˆå­¦ç”Ÿç­–ç•¥å¤´ï¼‰
std æˆ– log_stdï¼ˆåŠ¨ä½œå™ªå£°å‚æ•°ï¼‰

resumeæ—¶å°±å¯ç›´æ¥åŠ è½½è®­ç»ƒå¾—åˆ°çš„ckptï¼Œé‡Œé¢åŒ…å«äº†æ•™å¸ˆå’Œå­¦ç”Ÿã€‚